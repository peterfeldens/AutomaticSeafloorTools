{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rmtzpkUQThtK"
   },
   "source": [
    "# Load modules and define functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "id": "9ne9J7LuO4qi",
    "outputId": "d3a2fc22-3f71-4456-df9c-10dddc183791"
   },
   "outputs": [],
   "source": [
    "##Import stuff\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "import pyproj\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from osgeo import gdal, ogr, osr\n",
    "import sqlite3\n",
    "import random\n",
    "from shapely import wkt\n",
    "import glob\n",
    "import gdal\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RGWb9DO7Tef5"
   },
   "outputs": [],
   "source": [
    "def get_boundaries(image):\n",
    "    '''\n",
    "    Bestimmen der Bildgrenzen\n",
    "    '''\n",
    "    import gdal\n",
    "    src = gdal.Open(image)\n",
    "    ulx, xres, xskew, uly, yskew, yres = src.GetGeoTransform()\n",
    "    lrx = ulx + (src.RasterXSize * xres)\n",
    "    lry = uly + (src.RasterYSize * yres)\n",
    "    return ulx, xres, uly, yres, lrx, lry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bonU75CVGyD8"
   },
   "source": [
    "# Preparation of mosaics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aKHfzxGSO4qr"
   },
   "source": [
    "## Cutout of training images\n",
    "\n",
    "This can happen either\n",
    "- to cut a mosaic to be classified into small pieces\n",
    "- to try an object classification system \n",
    "- to train the super resolution model \n",
    "\n",
    "This needs to be done for each mosaic that was picked in QGIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ftLv-jJrO4qs"
   },
   "source": [
    "### Variables for cutting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZgRODkgRO4qt"
   },
   "outputs": [],
   "source": [
    "os.chdir('/home/peter/git/retina_seafloor')\n",
    "#mosaic_folder = './stone_dataset/GeoTIFS/Kriegers_Flak/mosaics_for_train/' #needs to end with /\n",
    "mosaic_folder = '/home/peter/git/retina_seafloor/stone_dataset/GeoTIFS/Kriegers_Flak/mosaics_for_test/'#needs to end with /\n",
    "input_mosaics = ['steintest2_25cm_upscaled_by_SR_25res.tif']\n",
    "#input_mosaics = ['1_BS_HOM_GR.png', '2_BS_HOM_GR.png' ]\n",
    "#target_folder = './tiles/Object_Detect/100m_25cm/'\n",
    "target_folder = '/home/peter/git/retina_seafloor/applydata/kw_test/25cm_upscaled_SR/'\n",
    "tile_size = '20 20'\n",
    "overlap_pixels = '2'\n",
    "shape_index_name = 'tiles.shape'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k88Vm872QZHT"
   },
   "source": [
    "### Do the cutting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "wIzqNnxWO4qz",
    "outputId": "03ddddbc-2315-470a-8539-53bfc8831bb0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gdal command for tiling images\n",
      "gdal_retile.py -ps 20 20 -targetDir /home/peter/git/retina_seafloor/applydata/kw_test/25cm_upscaled_SR/ /home/peter/git/retina_seafloor/stone_dataset/GeoTIFS/Kriegers_Flak/mosaics_for_test/steintest2_25cm_upscaled_by_SR_25res.tif\n"
     ]
    }
   ],
   "source": [
    "for element in input_mosaics:\n",
    "    # Therefore, this is the workaround with a call to the command line, which requires that gdal is installed on the PC. It is also available on google colab\n",
    "    cmd = 'gdal_retile.py -ps ' + tile_size + ' -targetDir ' + target_folder + \\\n",
    "            ' ' + mosaic_folder + element\n",
    "    print(\"gdal command for tiling images\")\n",
    "    print(cmd)\n",
    "    os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create tfw files if needed\n",
    "#funktioniert nicht f√ºr gedrehte bilder\n",
    "\n",
    "import osgeo.osr as osr\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "def generate_tfw(path, scale_factor = 4, gen_prj='prj'):\n",
    "    for infile in glob.glob(os.path.join(path, '*.tif')):\n",
    "        src = gdal.Open(infile)\n",
    "        xform = src.GetGeoTransform()\n",
    "\n",
    "        if gen_prj == 'prj':\n",
    "            src_srs = osr.SpatialReference()\n",
    "            src_srs.ImportFromWkt(src.GetProjection())\n",
    "            src_srs.MorphToESRI()\n",
    "            src_wkt = src_srs.ExportToWkt()\n",
    "\n",
    "            prj = open(os.path.splitext(infile)[0] + '.tif.png.tif.prj', 'wt')\n",
    "            prj.write(src_wkt)\n",
    "            prj.close()\n",
    "\n",
    "        src = None\n",
    "        corr_1 = xform[1] / scale_factor\n",
    "        corr_5 = xform[5] / scale_factor\n",
    "        edit1=xform[0]+corr_1/2\n",
    "        edit2=xform[3]+corr_5/2\n",
    "\n",
    "        tfw = open(os.path.splitext(infile)[0] + '.tif.png.tif.tfw', 'wt')\n",
    "        tfw.write(\"%0.8f\\n\" % corr_1)\n",
    "        tfw.write(\"%0.8f\\n\" % xform[2])\n",
    "        tfw.write(\"%0.8f\\n\" % xform[4])\n",
    "        tfw.write(\"%0.8f\\n\" % corr_5)\n",
    "        tfw.write(\"%0.8f\\n\" % edit1)\n",
    "        tfw.write(\"%0.8f\\n\" % edit2)\n",
    "        tfw.close()\n",
    "        \n",
    "generate_tfw('/home/peter/git/SRGAN-tensorflow/data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5zV0kEFlO4q5"
   },
   "source": [
    "## Create test/train csv files with class annotation and link to the exported tiles. \n",
    "\n",
    "This will read QGIS sqslite databases with coordinates stored in WKT formats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_g3O3e0rQlhQ"
   },
   "source": [
    "### Variables for class annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wCPLvGYGO4q6"
   },
   "outputs": [],
   "source": [
    "os.chdir('/home/peter/git/retina_seafloor')\n",
    "\n",
    "image_folder = './tiles/Object_Detect/25m_25cm_multires/' #end with /   folder with tiles \n",
    "\n",
    "image_type = '.tif'\n",
    "\n",
    "databases = ['./stone_dataset/Annotations/v2pickedstone.sqlite',\n",
    "            './stone_dataset/Annotations/v2empty.sqlite'] #sqlite with picked objects\n",
    "\n",
    "table_names = ['v2pickedstone',\n",
    "              'v2empty'] # table name to use in the sqlite file, normally the filename without sqlite at the end\n",
    "\n",
    "buffers = ['no',\n",
    "          'yes']  # yes when points are imported, create recangular buffer around dots. \n",
    "\n",
    "buffer_value = 1  #buffer size in m\n",
    "\n",
    "should_we_add_class = 'yes' # add a class name to the data?\n",
    "\n",
    "class_names = ['stone',\n",
    "              'empty']   # empy exmaples for retinanet-training go by ''empty''\n",
    "\n",
    "appendix_for_annotation_file = '100x100_25cm_stones_empty_3band'\n",
    "\n",
    "export_for_ = 'retinanet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5bNAc1IlQvIr"
   },
   "source": [
    "### Code for variable assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "XJGijUqBO4q8",
    "outputId": "a754745d-9c1f-45de-c76d-3826eff3f8bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on:  ./stone_dataset/Annotations/v2pickedstone.sqlite stone v2pickedstone\n",
      "Working on:  ./stone_dataset/Annotations/v2empty.sqlite empty v2empty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda2/envs/GIS/lib/python3.6/site-packages/ipykernel_launcher.py:124: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/peter/anaconda2/envs/GIS/lib/python3.6/site-packages/ipykernel_launcher.py:125: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/peter/anaconda2/envs/GIS/lib/python3.6/site-packages/ipykernel_launcher.py:126: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/peter/anaconda2/envs/GIS/lib/python3.6/site-packages/ipykernel_launcher.py:127: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/peter/anaconda2/envs/GIS/lib/python3.6/site-packages/ipykernel_launcher.py:134: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/peter/anaconda2/envs/GIS/lib/python3.6/site-packages/ipykernel_launcher.py:135: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/peter/anaconda2/envs/GIS/lib/python3.6/site-packages/ipykernel_launcher.py:136: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/peter/anaconda2/envs/GIS/lib/python3.6/site-packages/ipykernel_launcher.py:137: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "assert len(databases) == len(class_names) == len(table_names)\n",
    "\n",
    "# einlesen der sqlite datenbank mit den gepickten rechtecken oder punkten\n",
    "\n",
    "data = [] #make empty result list\n",
    "for i, (database, class_name, table_name, should_we_buffer) in enumerate(zip(databases, class_names, table_names, buffers)):\n",
    "    print('Working on: ' , database, class_name, table_name)\n",
    "    cn = sqlite3.connect(database)\n",
    "    sql_query = \"SELECT * FROM \" + table_name\n",
    "    vector_df = pd.read_sql_query(sql_query, cn)\n",
    "\n",
    "    #Create geopandas\n",
    "    vector_df['Coordinates'] = vector_df['WKT_GEOMETRY'].apply(wkt.loads)\n",
    "    vector_gdf = geopandas.GeoDataFrame(vector_df, geometry='Coordinates')\n",
    "    # get image list\n",
    "    image_list = glob.glob(image_folder + '*' + image_type)\n",
    "\n",
    "\n",
    "    results = []\n",
    "    for image in image_list:\n",
    "        ulx, xres, uly, yres, lrx, lry = get_boundaries(image)\n",
    "        results.append([image, ulx, uly, lrx, lry, xres, yres])\n",
    "\n",
    "    raster_df = pd.DataFrame(results, columns = ['image', 'ulx', 'uly', 'lrx', 'lry', 'xres', 'yres'])\n",
    "\n",
    "    # add emtpy imagename columns to vector gdf\n",
    "    vector_gdf['image_path'] = ''\n",
    "\n",
    "    if should_we_add_class == 'yes':\n",
    "        vector_gdf['classname'] = class_name\n",
    "\n",
    "    ##Compare the two data frames\n",
    "    for row in vector_gdf.itertuples():\n",
    "        #bounds return (minx, miny, maxx, maxy)\n",
    "        idx = row.Index\n",
    "        minx = float(row.Coordinates.bounds[0])\n",
    "        miny = float(row.Coordinates.bounds[1])\n",
    "        maxx = float(row.Coordinates.bounds[2])\n",
    "        maxy = float(row.Coordinates.bounds[3])\n",
    "\n",
    "        if should_we_buffer == 'yes':\n",
    "            #this allows \"rebuffer\"\n",
    "            mean_x = (float(minx) + float(maxx)) /2\n",
    "            mean_y = (float(maxy) + float(miny))/2\n",
    "\n",
    "            minx = mean_x - buffer_value/2\n",
    "            maxx = mean_x + buffer_value/2\n",
    "            miny = mean_y - buffer_value/2\n",
    "            maxy = mean_y + buffer_value/2\n",
    "\n",
    "\n",
    "        image_exists = []\n",
    "        image_exists = raster_df[(raster_df.ulx.values < minx) & (raster_df.uly.values > maxy) & (raster_df.lrx.values > maxx) & (raster_df.lry.values < miny )]\n",
    "        if not image_exists.empty:\n",
    "            for image in image_exists.iterrows():\n",
    "                # get unqie list of objects/images and calculate pixels\n",
    "                image_xmin = image[1].ulx\n",
    "                image_ymax = image[1].uly\n",
    "\n",
    "                image_res_y = image[1].yres\n",
    "                image_res_x = image[1].xres\n",
    "\n",
    "\n",
    "                pixel_ymin = np.abs(int((image_ymax - maxy) / image_res_y))\n",
    "                pixel_ymax = np.abs(int((image_ymax - miny) / image_res_y))\n",
    "                pixel_xmin = np.abs(int((minx - image_xmin) / image_res_x))\n",
    "                pixel_xmax = np.abs(int((maxx - image_xmin) / image_res_x))\n",
    "\n",
    "\n",
    "\n",
    "                if pixel_ymin == pixel_ymax:\n",
    "                    print('Image resolution to low for picked data, skipping entry (ymin == ymax)')\n",
    "                    continue\n",
    "                if pixel_xmin == pixel_xmax:\n",
    "                    print('Image resolution to low for picked data, skipping entry (xmin == xmax')\n",
    "                    continue\n",
    "\n",
    "                data.append(dict({'classname': row.classname, 'imagename' : image[1].image, 'minx' : minx, 'miny' : miny, 'maxx' : maxx, 'maxy' : maxy, 'pixelxmax': pixel_xmax, 'pixelxmin' : pixel_xmin, 'pixelymax' : pixel_ymax, 'pixelymin': pixel_ymin}))\n",
    "\n",
    "\n",
    "#Make annotated csv filenames\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if export_for_ == 'tfrecord':\n",
    "# Reformat for usage with Tensorflow API and no longer Keras Retinanet\n",
    "  df['width'] = df.maxx - df.minx\n",
    "  df['height'] = df.maxy - df.miny\n",
    "  columns = ['imagename', 'width', 'height', 'classname', 'pixelxmin', 'pixelymin', 'pixelxmax', 'pixelymax']\n",
    "\n",
    "  # For tfrecords we have to remove the paths, because we have to give the image dir directly \n",
    "  df.imagename = df.imagename.apply(lambda x: os.path.split(x)[1])\n",
    "\n",
    "\n",
    "  # Export Test and Train sets\n",
    "  train, test = train_test_split(df, test_size=0.2)\n",
    "  train.to_csv(appendix_for_annotation_file + 'train.csv', header = None, index = None, sep = ',', columns = columns)\n",
    "  test.to_csv(appendix_for_annotation_file+ 'test.csv', header = None, index = None, sep = ',',  columns = columns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mask = (df['classname'] == 'empty')\n",
    "df.at[mask, 'minx'] = 99999\n",
    "\n",
    "if export_for_ == 'retinanet':\n",
    "# Reformat for usage with Tensorflow API and no longer Keras Retinanet\n",
    "\n",
    "\n",
    "\n",
    "  columns = ['imagename', 'pixelxmin', 'pixelymin', 'pixelxmax', 'pixelymax', 'classname']\n",
    "\n",
    "  # Export Test and Train sets\n",
    "  train, test = train_test_split(df, test_size=0.2)\n",
    "  train.to_csv(appendix_for_annotation_file + 'train.csv', header = None, index = None, sep = ',', columns = columns)\n",
    "  test.to_csv(appendix_for_annotation_file+ 'test.csv', header = None, index = None, sep = ',',  columns = columns)\n",
    "\n",
    "  #resolve empty classes ..only for retinannet \n",
    "  columns = ['imagename', 'pixelxmin', 'pixelymin', 'pixelxmax', 'pixelymax', 'classname']\n",
    "\n",
    "  df = pd.read_csv(appendix_for_annotation_file + 'train.csv', sep=',', header=None, names = columns)\n",
    "  df.pixelxmax[df.classname == 'empty'] = str()\n",
    "  df.pixelymin[df.classname == 'empty'] = str()\n",
    "  df.pixelymax[df.classname == 'empty'] = str()\n",
    "  df.pixelxmin[df.classname == 'empty'] = str()\n",
    "  df.classname[df.classname == 'empty'] = str()\n",
    "  df.to_csv(appendix_for_annotation_file + 'train.csv', header = None, index = None, sep = ',', columns = columns)\n",
    "\n",
    "\n",
    "\n",
    "  df = pd.read_csv(appendix_for_annotation_file + 'test.csv', sep=',', header=None, names = columns)\n",
    "  df.pixelxmax[df.classname == 'empty'] = str()\n",
    "  df.pixelymin[df.classname == 'empty'] = str()\n",
    "  df.pixelymax[df.classname == 'empty'] = str()\n",
    "  df.pixelxmin[df.classname == 'empty'] = str()\n",
    "  df.classname[df.classname == 'empty'] = str()\n",
    "  df.to_csv(appendix_for_annotation_file + 'test.csv', header = None, index = None, sep = ',', columns = columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1oQS2pvqlWzW"
   },
   "source": [
    "## Super Resolution\n",
    "\n",
    "\n",
    "Training Instructions:\n",
    " - export the high-res model into tiles of 100x100 /see above\n",
    " - we use folder ./super_res_dataset/train_HR for that\n",
    " - the script below will create the training and validation datasets\n",
    " \n",
    " - copy fhe files manually to png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700
    },
    "colab_type": "code",
    "id": "6Q8X6WX-laiB",
    "outputId": "dde3a925-4bdf-4a14-f748-fcf201c6386d"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "os.chdir('/home/peter/git/retina_seafloor')\n",
    "HR_train = '/home/peter/git/SRGAN-tensorflow/result/images/'    #This is used as the origin folder. The other folders are hardwired\n",
    "HR_valid = './super_res_dataset/valid_HR/'    # folder must end with /\n",
    "factor = 4\n",
    "\n",
    "def img_convert(image, image_format = '.png'):\n",
    "    #from skimage.io import imread \n",
    "    #from skimage.io import imsave\n",
    "    #img = imread(image, as_gray=True)\n",
    "    #imsave(image + image_format, img)\n",
    "    from PIL import Image\n",
    "    try:\n",
    "        img = Image.open(image)\n",
    "        img.save(image + image_format)\n",
    "    except:\n",
    "        print('no Image')\n",
    "    return\n",
    "\n",
    "def downsampling(folder,image_name, factor, target_folder):\n",
    "    from skimage.transform import rescale\n",
    "    from skimage.io import imsave\n",
    "    from skimage.io import imread\n",
    "    \n",
    "    image = folder + image_name\n",
    "    img = imread(image, as_gray=True)\n",
    "    img_down = rescale(img, 1/factor)\n",
    "    #img_up = rescale(img_down, factor)\n",
    "    imsave(target_folder + image_name, img_down)\n",
    "    return \n",
    "    \n",
    "\n",
    "def random_distribution(source_folder, target_folder, valid_percentage = 0.2):\n",
    "    import os\n",
    "    import random\n",
    "    import shutil\n",
    "\n",
    "    filelist = os.listdir(source_folder)  # We assume this is the HR_train folder\n",
    "    \n",
    "    number_of_files = len(filelist)\n",
    "    print(number_of_files)\n",
    "    \n",
    "    index_of_files =  range(0,number_of_files-1)\n",
    "    print(index_of_files)\n",
    "    \n",
    "    number_of_valid_files = int(number_of_files * valid_percentage)\n",
    "    \n",
    "    random_files_index = random.sample(index_of_files, number_of_valid_files)\n",
    "    for i in random_files_index:\n",
    "        shutil.move(source_folder + filelist[i], target_folder + filelist[i])\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "#convert to png\n",
    "filelist = os.listdir(HR_train)\n",
    "print(filelist[0])\n",
    "for image in filelist:\n",
    "    img_convert(HR_train + image, '.tif')\n",
    "    #command = \"rm \" + HR_train + image\n",
    "    #os.system(command)\n",
    "\n",
    "    \n",
    "\n",
    "#distrubte randomly to validate\n",
    "#random_distribution(HR_train, HR_valid)\n",
    "\n",
    "\n",
    "# downsample\n",
    "#filelist = os.listdir(HR_train)\n",
    "#for image in filelist:\n",
    "#    downsampling(HR_train, image, factor, './super_res_dataset/train_LR/' )\n",
    "\n",
    "#filelist = os.listdir(HR_valid)\n",
    "#for image in filelist:\n",
    "#    downsampling(HR_valid, image, factor,'./super_res_dataset/valid_LR_x4/' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "x-R1OWAYO4q-",
    "outputId": "b098ecc4-865f-41d7-a228-736a257f96b0"
   },
   "outputs": [],
   "source": [
    "# Enhance! This!\n",
    "os.chdir('/home/peter/git/SRGAN-tensorflow')\n",
    "command = 'sh test_SRResnet.sh'\n",
    "os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computer, transferiere di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conver to tif\n",
    "\n",
    "#make sure its gray\n",
    "for i in *.png; do convert \"$i  -set colorspace Gray -separate -average \" \"${i%.*}.png\"; done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "H4w6yRdP3ArH",
    "outputId": "c6af278f-7d3d-4d5a-cbac-f5ebb1346ec1"
   },
   "outputs": [],
   "source": [
    "#generate new world files from low resolution data (see above) with scale factor applied an copy in results image folder\n",
    "\n",
    "# Merge da tile\n",
    "gdal_merge.py -o test.tif  *.tif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "2yW6SnxZKfT-",
    "outputId": "b51b519d-9e80-4f3d-e207-c8718fd56fb5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pi6f0D0pO4rN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "aKHfzxGSO4qr",
    "Dp0zr5v9O4q-"
   ],
   "name": "Seafloor_Classification_Preparation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
